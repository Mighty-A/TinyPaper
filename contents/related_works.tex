\chapter{相关工作}

本章我们将介绍和本文研究主题相关的现有研究进展，主要包括两个方面：联邦学习框架下实现高效通信方法、联邦学习中现有的隐私保护方法。

\section{联邦学习实现高效通信的方法}


通信问题作为制约联邦学习训练效率的主要瓶颈之一，已经引起了研究者的广泛关注。针对这一问题，早期的一些研究\cite{konevcny2016federated}中表明，结合联邦平均法（Federated Averaging，FedAvg）\cite{mcmahan2017communication}和一些模型压缩量化/稀疏化的方法，能够将联邦学习中通信开销显著降低，同时避免对于模型精度产生较大影响。然而，由于联邦学习应用场景下，各类终端设备、无线连接的传输速率、稳定性、价格成本等条件和传统数据中心内分布式框架相比有着巨大的差距，因而研究者们关心能否更进一步降低模型的通信开销，或在合理的精度损失下，更大程度上降低模型通信量，即找到联邦学习框架下精度和通信量的最佳平衡点。

在理论研究上，如何在统计估计、机器学习中权衡通信量和精度的问题也一直引起了研究者们的关注。\parencite{zhang2013information, braverman2016communication, han2018geometric, barnes2020lower}等工作从信息论的角度给出了在一定通信量限制下的统计估计能达到的极小极大下界（minimax lower bound）。然而，这些理论上的研究结果难以直接给出，在联邦学习框架下，使通信量显著下降的指导，这是因为这些理论上的研究通常忽略了联邦学习下优化算法的特点及影响。

针对联邦学习的场景，考虑到客户端终端设备资源的有限性，实现高效通信有着以下三个压缩目标：
\begin{enumerate}[label=(\alph*)]
    \item \label{enm:objectives:gradient_compression} 模型更新压缩：降低客户端上传模型更新（梯度/权重）至服务端所需的通信量
    \item 模型广播压缩：降低服务端广播发送全局更新后的模型至客户端所需的通信量
    \item 本地计算缩减：降低本地计算和时间开销，以降低每一轮训练中由于本地计算过程带来的通信延迟。
\end{enumerate}

在现有的研究中，这三个压缩目标往往是互补的，即在一定方法下，三个目标均有可能成为降低系统通信开销的短板。而在这三个压缩目标中，\ref{enm:objectives:gradient_compression}的实现对于高效通信的影响最为显著。这是因为客户端连接的下载速度往往高于上传速度，使得上传模型更新的过程更容易成为系统通信的瓶颈；此外，联邦学习中模型聚合特点也使得实现目标\ref{enm:objectives:gradient_compression}的方法更加多样。

为了实现压缩目标\ref{enm:objectives:gradient_compression}，\parencite{lin2017deep}采用对本地上传的梯度进行稀疏化，根据模型权重的重要性有选择地进行梯度传输。\parencite{jiang2022model}在初始模型和联邦学习过程中引入剪枝操作，随着训练过程降低模型大小。\parencite{kwon2022efficient}则通过不同的降维方法，将具有较多参数的模型降低到低维子空间（low-dimensional subspace）。\parencite{wu2016quantized}采取将CNN（convolutional nerual network）的模型参数进行量化，达到降低模型大小的效果。

值得注意的是，部分模型压缩技术在应用到结合了差分隐私的联邦学习时会遇到一些兼容性问题，如标准量化操作与差分隐私为不同客户端加入不同噪声之间并不兼容\cite{kairouz2021advances}。

\section{联邦学习中的隐私保护方法}

相比中心化的机器学习系统，联邦学习框架下客户端本地数据始终不会离开客户端设备，这一结构上的特点天然地为客户端数据带来了一定程度上的隐私性。然而，这一特点本身并不足以提供针对客户端形式化的隐私保护。比如，攻击者可以通过窃听上传的模型参数更新来推断客户端的个体隐私信息。

分析联邦学习的隐私问题，可将联邦学习视为一种特殊的分布式数据分析系统，即数据分析工作者将某一统计函数$f$应用到从不同的数据源，整合得到结果。为了实现在这一过程中的隐私保护，现有的研究工作从三个不同的方面展开\cite{kairouz2021advances}：如何计算$f$，$f$计算何物，如何验证$f$的计算。

针对如何计算$f$的问题，即如何设计统计、聚合函数$f$来保护用户隐私，现有的研究多从安全计算（Secure Computations）的角度出发，在联邦学习中应用了基于同态加密（Homomorphic Encryption）的安全多方计算（Secure Multi-Party Computation，MPC）\cite{yao1986generate}、基于特定硬件的可信执行环境（Trusted Execution Environments）\cite{subramanyan2017formal}等方法。

针对$f$计算何物的问题，即确定多少比例用户的信息被暴露到系统中用于计算，现有的研究主要通过在联邦学习中应用差分隐私方法来保护隐私。通过在直接使用客户端数据集的过程中应用差分隐私机制，即本地数据集上的差分隐私，使得用户本地数据集得到保护，然而这种方式往往极大地影响了数据可用性。\parencite{geyer2017differentially}提出基于全局差分隐私的联邦学习框架，在每一轮训练时，服务端随机选取一组客户端进行训练，收取各个客户端的模型更新，并在聚合模型更新时加入高斯噪声，进而使得攻击者无法从广播阶段发布的共享模型中获取客户端的隐私信息。但是，由于服务端能获得各个客户端上传的模型更新，这种方法无法防范恶意的服务端。在此基础上，\parencite{abadi2016deep, mcmahan2017learning, zheng2020preserving}提出用户级差分隐私（user-level differential privacy），通过将用户上传的梯度裁切、加入高斯噪声后聚合，结合一定隐私预算（privacy budget）机制，避免模型过拟合至用户个体数据上。

针对如何验证$f$的结果问题，即确保客户端和服务端均按照规定的协议执行操作，现有的研究采用零知识证明（Zero-knowledge proofs，ZKP）\cite{goldwasser2019knowledge, parno2016pinocchio}这一密码学原语（primitive）进行联邦学习框架下服务端对客户端行为的验证。

在与本文讨论重点相关的结合联邦学习的差分隐私研究中，往往会遇到在模型精确度和加入噪声强度间平衡的问题，且该问题在大模型上愈发凸显，因为大模型意味着传输更多的参数，需要更强的噪声来保护隐私\cite{mcmahan2017learning}。

\section{本章小结}

