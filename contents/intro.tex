% !TEX root = ../main.tex

\chapter{绪论}

本文中我们将介绍联邦学习（Federated Learning）中通信效率（communication efficiency）和隐私保护（privacy preservation）两个方面的问题，并通过基于模型训练轨迹的降维压缩方法尝试解决这两个方面的问题。这两个问题均是当前联邦学习研究领域广受关注的诸多问题之一。本文的组织结构如下，首先我们将在本章中介绍联邦学习、模型压缩、差分隐私的相关背景知识。在第二章中，我们将讲述联邦学习框架下实现高效通信和隐私保护的相关工作，对比联邦学习下各类模型压缩方法和隐私保护策略等。然后，我们将在第三章详细介绍本文所应用的基于模型训练轨迹的降维压缩方法，包括这一方法与以往模型压缩方法的主要区别和在中心化场景下的实验。第四章中，我们将介绍将这一降维压缩方法在联邦学习中具体的应用以及详细的实验结果。第五章我们将介绍如何在应用了降维方法的联邦学习框架中实现差分隐私方法，并给出详细的实验。最后，我们将给出整个论文的结论，总结我们方法的使用，并谈及一些未来的方向。

\section{联邦学习介绍}

\subsection{联邦学习的背景}

基于大数据的人工智能（artificial intelligence，AI）正在被应用到我们生活的方方面面。而智能手机、可穿戴设备、自动驾驶汽车等物联网（Internet of Things，IoT）设备正在每天产出大量的数据\cite{li2020federated}。在将人工智能整合到这些物联网应用的过程中，由于研究者们对数据传输效率、数据隐私安全的考虑，边缘计算（edge computing）和分布式机器学习（distributed machine learning）越来越受到关注。联邦学习\cite{mcmahan2017communication}（federated learning，FL）则凭借着其将数据保存在本地进行训练、由云端进行模型整合的能力，成为了分布式机器学习领域的前沿话题。


\subsection{联邦学习的框架}

常规的联邦学习框架包含一个服务端（云端）和$N$个客户端（本地端）如（TODO）所示。每个客户端$C_i$上均保存有一定大小的、与其他客户端不同的本地数据集$D_i$。服务端的目标为训练一个训练数据覆盖所有客户端的数据集的模型。每个参与联邦学习训练的客户端$C_i$，需要在本地数据集$D_i$计算得到最小化损失函数（loss function）的神经网络模型参数$w_i$。为了实现在涵盖所有客户端数据上最优，服务端需要对$N$个客户端上传的模型进行聚合（aggregation），聚合的公式如下：
\begin{equation}
 \mathbf{w} = \sum_{i = 1} ^ {N} p_i\mathbf{w}_i,
\end{equation}

其中，$N$为客户端的总数，$\mathbf{w}_i$为本地客户端$C_i$上传的模型参数，$\mathbf{w}$为服务端得到的聚合后模型参数，$p_i$为每个客户端本地数据集$D_i$占整体$D$的比例，定义为$p_i=\frac{|D_i|}{|D|}$，故有$\sum_{i = 1}^Np_i = 1$。综合最小化损失函数和聚合过程，我们用以下公式表述联邦学习下的优化问题：

\begin{equation}
  \mathbf{w}^* = \arg\min_{\mathbf{w}}\sum_{i = 1}^{N}p_iF_i(\mathbf{w}, D_i),
  \label{eq:optimization_problem}
\end{equation}

其中，$F_i$为第$i$个客户端的损失函数。

通常情况下，联邦学习框架下的一轮训练过程为：

\begin{enumerate}
  \item 本地训练：所有客户端根据自身存储的数据计算梯度或参数，并上传本地训练的模型至服务端
  \item 模型聚合：服务端通过安全聚合方法（secure aggregation），将$N$个客户端的模型聚合为全局模型。
  \item 参数广播：服务端将聚合后的模型参数广播给所有的客户端
  \item 模型更新：所有的客户端根据聚合的模型参数将自身的模型进行更新，并测试更新后的模型性能。
\end{enumerate}

在经过足够轮次的迭代更新和参数交换后，模型在满足一定条件下将收敛到优化问题\ref{eq:optimization_problem}的全局最优\cite{li2020federated}。

值得注意的是，在联邦学习框架的一轮迭代中，客户端除了可以上传本地计算更新后的模型参数，也可以上传本地计算的梯度或模型更新（model update），由服务端聚合各个客户端上传的梯度$\mathbf{g}_i$得到总梯度$\mathbf{g}$来更新服务端模型$\mathbf{w}$\cite{wei2020federated}。

\subsection{联邦学习面对的挑战}

随着对联邦学习的研究逐渐深入，一些关键的挑战也进入了研究者的视野，比如模型上传过程中的隐私信息泄露、云端和本地间的通信成本等问题\cite{kairouz2021advances}。

尽管藉由本地训练，客户端的数据能够避免被直接用于与云端进行交互，而是以本地计算的模型参数、梯度等形式被上传。但是通过分析本地上传的模型参数、梯度等信息，攻击者仍然能够窃取本地数据集中的隐私信息\cite{li2020federated, wei2020federated}。

与此同时，通信问题越来越成为制约联邦学习发展的瓶颈。参与联邦学习的设备数量大、通信轮数多，本地设备的上传、下载带宽（bandwidth）存在物理限制，且传输的模型随着神经网络的发展越来越庞大，这些因素使得联邦学习训练过程中通信时间占比较大、传输效率较低\cite{lin2017deep}。如图（TODO：Deep gradient）所示，随着参与联邦学习的节点数量增加，通信时间占总训练时间的占比显著增加。


\section{模型压缩介绍}

为了缓解联邦学习中通信开销巨大的问题，我们将采取模型压缩的策略降低每轮中传输模型参数、梯度的通信量。本小节将对模型压缩相关内容进行简要介绍。

\subsection{模型压缩的背景}

近年来，深度神经网络（Deep Neural Network，DNN）在各个应用领域大放异彩。然而，在许多应用领域的工作，依赖参数规模庞大的神经网络，如表\ref{tab:dnn_param}中给出了常见的神经网络的参数量。这些网络需要大量的CPU、GPU计算资源，且不便于迁移到现今广泛使用的手机等移动设备中。

\begin{table}[!hpt]
  \caption{常见深度神经网络的参数量}
  \label{tab:dnn_param}
  \centering
  \begin{tabular}{@{}lr@{}} \toprule
    模型名称  & 参数量 \\ \midrule
    VGG11\cite{simonyan2014very}  & 28.5M \\
    MobileNet\cite{howard2017mobilenets}     & 3.3M \\
    Xception\cite{chollet2017xception}    & 21.0M \\
    Inceptionv3\cite{szegedy2016rethinking} & 22.3M \\ 
    Resnet20\cite{he2016deep} & 0.27M\\
    \bottomrule
  \end{tabular}
\end{table}

在神经网络的许多应用场景中，都对于在实现同样或者接近性能的情况下尽可能少的模型参数量有着极大的需求。在模型训练阶段，更少的参数量意味着更低的GPU计算资源开销，能够加快计算速度和减少电力能源消耗。在模型推理阶段，更少的参数量意味着更快地完成一次前向推理过程，能显著降低神经网络响应的延时，是自动驾驶等领域主要需求之一。在分布式机器学习场景下，更小的模型意味着更少的存储空间和更快的模型分发效率。

因此，将庞大的深度神经网络进行压缩，使得其能够在更小的资源消耗下进行训练或推理，便显得愈发重要。在本文所主要涉及的联邦学习框架下，客户端和服务端需要进行较多轮的通信，传输模型参数或梯度信息，因而采取合适的模型压缩策略降低传输通信量，将显著提升联邦学习的训练性能。

\subsection{模型压缩的分类}

目前，在模型压缩领域常用的方法可以大致分成四类\cite{cheng2017survey}：剪枝与量化（parameter pruning and quantization）、低秩因子分解（low-rank factorization）、迁移/压缩卷积滤波器（transferred/compact convolutional filters）、知识蒸馏（knowledge distillation）

其中，剪枝通过给组成模型的单元（权重、神经元、卷积核等）按照重要性排序，删除重要性较低的部分；而量化则通过减少模型权重的比特数（the number of bits）来降低模型大小，如将32位浮点数转换成8-bit甚至更小的表示。这一类方法注重于删减模型参数中的冗余信息，适合不同的架构且同时支持从头训练和预训练模型。

低秩因子分解则通过矩阵/张量分解的方式，对神经网络中重要的参数进行估计，降低存储模型参数所需要的维度，进而进行模型压缩。实际应用中，通常采用逐层的方式将卷积层、全连接层进行低秩因子分解，对矩阵采用满秩分解（full-rank decomposition）和奇异值分解（singular value decomposition，SVD）\cite{denton2014exploiting}，对高维张量可采用CP分解（Canonical Polyadic）\cite{lebedev2014speeding}。这一类方法也同样支持从头训练和预训练模型。

迁移/压缩卷积滤波器则是受到\parencite{cohen2016group}提出的CNN上的群等变理论（the equivariant group theory）启发，通过设计具有特殊结构的卷积滤波器，降低每个滤波器所需的参数量，达到压缩CNN结构的深度神经网络的目的。这一类压缩方法的效果和目标任务高度相关，且只能用于从头训练神经网络的情况，无法从现有预训练模型中提取权重。

知识蒸馏\cite{hinton2015distilling}方法，通过将一个在大数据集上训练的复杂网络（通常称为教师模型，teacher model）作为监督信号，训练一个更小、更轻量级的模型（也称为学生模型，student model）,使学生模型尽可能接近教师模型的精度和性能，以此来达到同样性能下模型参数更少的目的。这一类方法的效果也会受到具体下游任务精度的影响，且只能用于从头训练神经网络。

在这四类常见的模型压缩方法之外，还有着许多研究进行着类似的尝试\cite{hinton2015distilling,almahairi2016dynamic, shazeer2017outrageously}。诸多的模型压缩方法均有着各自的压缩特点和各自的适用范围，需要结合具体的任务场景选择合适的方法且对其具体使用进行调整。


\section{差分隐私介绍}

为了在联邦学习框架下保护客户端的数据隐私，我们将采用差分隐私方法避免客户端上传的模型参数、梯度中的隐私泄露。本小节将介绍联邦学习下隐私攻击模型和差分隐私相关的知识。

\subsection{隐私攻击模型}

在联邦学习框架中，虽然第$i$ 个客户端的数据集$D_i$始终保存在客户端本地，但其本地训练得到的参数$\mathbf{w}_i$或梯度$\mathbf{g}_i$需要上传到云端，这些上传的信息中极有可能会揭示本地的隐私信息。比如，\parencite{fredrikson2015model}提出的模型反转攻击（Model inversion attack）能够从一个采用了深度神经网络的面部识别系统的权重参数中恢复出人脸图像，因而在联邦学习框架下，客户端上传的模型参数易受到攻击。与此同时，在参数广播阶段，服务端向所有客户端广播的聚合后模型参数$\mathbf{w}$同样也可能被用于分析参与的客户端隐私信息。

考虑到联邦学习中上述的隐私风险，本文所讨论的联邦学习隐私攻击模型为：假定各个客户端和服务端均为诚实的（honest），但存在第三方攻击者，能够窃听各个客户端上传的模型参数、梯度以及服务端广播的模型参数。

\subsection{差分隐私}

应对上述隐私攻击模型的常用方法为在本地上传的信息中加入人造噪声，而如何加入噪声、加入多少噪声变成隐私保护的一大核心问题。对于这一问题的典型方法方法之一便是差分隐私（Differential Privacy）。

差分隐私中的$(\epsilon, \delta)$-DP\cite{dwork2014algorithmic}为分布式数据处理系统提供了强有力的隐私保证。$(\epsilon, \delta)$-DP中，$\epsilon > 0$ 表示数据库中相邻数据集$D_i,D_i'$所有输出的可区分边界，即隐私预算；$\delta$ 表示在加入某种隐私保护机制后，相邻数据集$D_i$和$D_i'$分布不在$e^\epsilon$范围内的松弛项，即差分隐私在一定程度上的不满足。对于任意给定的$\delta$，有着更大的隐私预算$\epsilon$的隐私保护机制会导致更大的相邻数据集输出差异，也更容易发生隐私泄露。差分隐私形式化的表述为：

\begin{definition}[ $(\epsilon, \delta)$-DP]
  定义在$X\rightarrow R$上的随机化机制$M: X \rightarrow R$ 要满足$(\epsilon, \delta)$-DP，仅当对$R$的所有可测集（measurable set）$S\subseteq R$ 和$X$的任意相邻子集$D_i, D_i'\in X$，有：

  \begin{equation}
    \text{Pr}\left[ M(D_i)\in S\right] 
    \le e^\epsilon \text{Pr}\left[ M(D_i')\in S\right] + \delta
  \end{equation}
\end{definition}

对于数值数据，定义在\parencite{dwork2014algorithmic}中的高斯机制（Gaussian mechanism）可以被用于保证$(\epsilon, \delta)$-DP。我们在这里给出以下采用加入高斯噪声实现的差分隐私机制。

为了使我们加入的高斯噪声$n\sim N(0, \sigma^2)$满足$(\epsilon, \delta)$-DP，我们将在$\epsilon \in [0, 1]$时选择噪声水平为$\sigma \ge c \Delta s / \epsilon$，其中常数$c \ge \sqrt{2ln(1.25/\delta)}$。在前述式子中，$n$表示在数据集的一个数据样本中加入的加法噪声采样值；$\Delta s$表示一个实数值域函数$s$的敏感度，定义为：$\Delta s = \max_{D_i, D_i'} \Vert s(D_i) - s(D_i')\Vert $。

在上述差分隐私机制下，不同的噪声水平会极大影响对客户端的隐私保护以及模型收敛能力。因此，在应用差分隐私时，需要结合实际场景选择合适的噪声水平。

\subsection{联邦学习框架下的全局差分隐私}

结合联邦学习框架和上述噪声机制，为了使得客户端上传的内容差分隐私，我们将在客户端上传的模型参数或梯度中加入高斯噪声。这里我们给出具体的加噪机制。

在客户端使用权重/梯度裁切（clipping）技术下，我们可以确保

\begin{equation}
  \Vert\mathbf{w}_i\Vert \le C,
\end{equation}

其中，$\mathbf{w}_i$表示第$i$个客户端上传的带有噪声扰动的参数，$C$则为限制$\mathbf{w}_i$数值大小的裁切阈值。假设本地训练的批量大小（batch size）和本地训练样本数相同，我们可以如下定义本地训练过程：

\begin{equation}
  \begin{aligned}
  s^{D_i} &\triangleq \mathbf{w}_i = \arg \min_{\mathbf{w}} F(\mathbf{w}, D_i) \\
      &=  \frac{1}{|D_i|}\sum_{j = 1}^{|D_i|} \arg \min_{\mathbf{w}} F(\mathbf{w}, D_{i,j}),
  \end{aligned}
\end{equation}

其中，$D_i$是第$i$个客户端的本地数据集，$D_{i, j}$是$D_i$的第$j$个样本。因此，$s^{D_i}$的敏感度可表示为：

\begin{equation}
  \begin{aligned}
  \Delta s^{D_i} &= \max_{D_i, D_i'} \Vert S^{D_i} - s^{D_i'}\Vert\\
  &= \max_{D_i, D_i'} \left\Vert \frac{1}{|D_i|}\sum_{j = 1}^{|D_i|} \arg \min_{\mathbf{w}} F(\mathbf{w}, D_{i,j}) - \frac{1}{|D_i'|}\sum_{j = 1}^{|D_i'|} \arg \min_{\mathbf{w}} F(\mathbf{w}, D_{i,j})'\right\Vert \\
  &= \frac{2C} {|D_i|}, 
  \end{aligned}
\end{equation}

其中，$D_i'$是和$D_i$大小相同、只有一个样本不同的相邻集（adjacent dataset），$D_{i,j}'$是$D_i'$的第$j$个样本。从上述结果可知，所有客户端上传本地训练的模型参数到服务端的全局总和敏感度为：

\begin{equation}
  \Delta s \triangleq \max\left\{\Delta s^{D_i}\right\}, \forall i.
\end{equation}

为了得到较小的全局敏感度，理想情况下每个客户端应提供足够的数据集用于本地训练。因此，我们用$m$表示各个客户端本地数据集的最小大小。从而，我们得到：
\begin{equation}
  \Delta s = \frac{2C}{m}.
\end{equation}

为了在一轮客户端上传的模型参数、梯度上保证$(\epsilon, \delta)$-DP，我们设定加入的噪声水平为:
\begin{equation}
  \sigma = \frac{c\Delta s}{\epsilon}
\end{equation}

若总共需要对本地数据集进行$L$次完整访问或总共需要经过$L$轮本地训练，即客户端需要上传$L$次本地训练得到的模型参数、梯度至服务端，则由于高斯噪声的线性性，我们加入的噪声水平为：
\begin{equation}
  \sigma = \frac{cL\Delta s}{\epsilon}
\end{equation}

\section{研究贡献总结}

\section{本章小结}
